\subsection{Probabilistic Language Model - Definition}
\begin{frame}{Formal definition}
	Let $\mathcal{V}$ be the vocabulary, a finite set of symbols or words. Let us use $\triangleleft$ and
	$\triangleright$ as the start and stop symbols and let them be the part of $\mathcal{V}$. Let $\left|\mathcal{V}\right|$ denote the size of  $\mathcal{V}$.
	\vspace{0.3cm}

	Let $W$ be infinite sequences of words from the collection of $\mathcal{V}$. Every sequence in $W$ starts with $\triangleleft$ and ends with $\triangleright$. Then a language model is a probability distribution of a random variable $\mathcal{X}$ which takes values from $W$. Or p: $W \rightarrow \mathbb{R}$ such that
	\begin{align}
		&\forall x \in W, p(x) \geq 0 \text{ and}\\
		&\sum_{x\in W}p(X=x) = 1
	\end{align}
\end{frame}

\begin{frame}{Probabilistic Language Model}
	\textbf{Goal}: Compute the probability of a sequence of words
	\begin{align}
		P(W)&= P(w_1,w_2,w_3,...w_n) \label{eq:seqwords}
	\end{align}
	\textbf{Task}: To predict the next word using probability. Given the context, find the next word using
	\begin{align}
		&P(w_n|w_1,w_2,w_3,\ldots,w_{n-1}) \label{eq:condprob}
	\end{align}
	A	model	which computes	the probability	for	\eqref{eq:seqwords} or predicting the next word \eqref{eq:condprob} or complete the partial sentence is called as  Probabilistic Language Model.


	\vspace{0.3cm}
	The  goal is to learn the joint probability function of sequences of words in a language.

	The probability of $P(\text{The cat roars})$ is less likely to happen than $P(\text{The cat meows})$\\
\end{frame}


\subsection{Chain Rule}
\begin{frame}{Chain Rule}

Is it difficult to compute the probability of the entire sequence $P(w_1,w_2,w_3,\ldots,w_n)$?
\linebreak
\textit{\textbf{Chain rule}} is used to decompose the joint probability of a sequence into a product of conditional probability
\begin{align}
P({W}) &= P(w_1,w_2,w_3,\ldots,w_n) = P(w_1^n)\\
&=P(w_1)P(w2|w_1)P(w3|w_2,w_1)\ldots P(w_n|w_{n-1},w_{n-2},w_{n-3},\ldots,w_1)\\
&=\prod_{k=1}^{n}P(w_k|w_1^{k-1})
\end{align}
\begin{itemize}
	\item It is possible to $P(w|h)$, but it does not really help in reducing the computational complexity
	\item We use innovative ways to string words to form new sentences
	\item Finding the probability for a long sentence may not yield good outcome as the context may never occur in the corpus
	\item Short sequences may provide better results
\end{itemize}

\end{frame}


\subsection{Markov Assumption}
\begin{frame}{Markov Assumption}
\textbf{\underline{Markov Assumption}}:The future behavior
of a dynamic system depends on its
recent history and not on the entire history
\linebreak
The product of the conditional probabilities can be written approximately for a bigram as
\begin{align}
P(w_k|w_1^{k-1}) &\approx P(w_k|w_{k-1}) \label{eq:MarkovBigram}
\end{align}
Equation (\ref{eq:MarkovBigram}) can be generalized for an \textit{n-gram} as
\begin{align}
P(w_k|w_1^{k-1}) &\approx P(w_k|w_{k-K+1}^{k-1}) \label{eq:MarkovNgram}
\end{align}
Now, the joint probability of a sequence can be re-written as
\begin{align}
P({W}) &= P(w_1,w_2,w_3,\ldots,w_n) = P(w_1^n)\\
&=P(w_1)P(w2|w_1)P(w3|w_2,w_1)\ldots P(w_n|w_{n-1},w_{n-2},w_{n-3},\ldots,w_1)\\
&=\prod_{k=1}^{n}P(w_k|w_1^{k-1}) \\
&\approx \prod_{k=1}^{n}P(w_k|w_{k-K+1}^{k-1})
\end{align}
\end{frame}
%\begin{frame}{End of session}
%\Large
%\begin{center}
%	END OF SESSION
%\end{center}
%\end{frame}

\subsection{Target and Context words}
\begin{frame}{Target and Context words}
Next word in the sentence depends on its immediate past words, known as context words
	\begin{equation}
		P(w_{k+1}|\underbrace{w_{i-k}, w_{i-k+1},\ldots,w_k}_{\mathclap{\text{Context words}}}) \nonumber
	\end{equation}
	n-grams

	\begin{tabular}{lll}
		unigram &- &$P(w_{k+1})$ \\
		bigram &- &$P(w_{k+1}|w_{k})$ \\
		trigram &- &$P(w_{k+1}|w_{k-1},w_k)$ \\
		4-gram &- &$P(w_{k+1}|w_{k-2},w_{k-1},w_k)$
	\end{tabular}

\end{frame}

\subsection{Language Modeling using Unigrams}
\begin{frame}{Language Modeling using Unigrams}
\begin{itemize}
	\item All words are generated independent of its history $W_,W_2,W_3,\ldots..W_n$ and none of them depend on the other
	\item Not a good model for language generation
	\item It will have $\left|V\right|$ parameters
	\item $ \theta_i = p(w_i) = \dfrac{c_{w_i}}{N}$, where $c_{w_i}$ if the count of the word $w_i$ and $N$ is the total number of words in the vocabulary
	\item It may not be able to pick up regularities present tin the corpus
	\item It is more likely to generate \textit{\textbf{the the the the}} as a sentence than a grammatically valid sentence

\end{itemize}

\end{frame}

\subsection{Generative Model}
\begin{frame}{Generative Model}
\begin{itemize}
	\item Generates a document containing $N$ words using n-gram
	\item A good model assigns higher probability to the word that actually occurs
\end{itemize}

\begin{equation}
	P(\mathbf{W}) = P(N)\displaystyle{\prod_{i=1}^{N}P(W_i)}
\end{equation}
\begin{itemize}
	\item The location of the word in the document is not important
	\item P(N) is the distribution over $N$ and is same for all documents. Hence it may be ignored
	\item  $W_i$, to be estimated in this model is $P(W_i)$ and it must satisfy $\sum_{i=1}^{N}P(w_i) = 1$
\end{itemize}


\end{frame}

\subsection{Maximum Likelihood Estimate}
\begin{frame}{Maximum Likelihood Estimate}
\begin{itemize}
	\item One of the methods to find the unknown parameter(s) is the use of Maximum Likelihood Estimate
	\item Estimate the parameter value for which the observed data has the highest probability
\end{itemize}
\begin{itemize}
	\item Training data may not have all the words in the vocabulary
	\item If a sentence with an unknown word is presented, then the MLE is zero.
	\item Add a smoothing parameter to the equation without affecting the overall probability requirements
\end{itemize}
\begin{align}
P(\mathbf{W}) &= \dfrac{C_{w_i}+\alpha}{C_W + \alpha|V|}\\
\text{If } \alpha &= 1,\text{then it is called as Laplace smoothing}\\
&P(\mathbf{W}) = \dfrac{C_{w_i}+1}{C_W + |V|}
\end{align}
\end{frame}

\subsection{Bigram Language Model}
\begin{frame}{Bigram Language Model}
	\begin{itemize}
		\item This model generates a sequence
		one word at a time, starting with the first word and then generating each
		succeeding word conditioned on the previous one or its predecessor
		\item A bigram language model or the Markov model (first order)is defined as follows:
		\begin{align}
		P(\textbf{W}) = \prod_{i=1}^{n+1}P(w_i|w_{i-1})
		\end{align}
		\item[] where $\textbf{W} = {w_1,w_2,w_3,\ldots,w_n}$
	\end{itemize}
\end{frame}

\begin{frame}{Bigram Language model}
	\begin{itemize}
		\item Estimate the parameter $P(w_i|w_{i-1})$ for all bigrams
		\item The parameter estimation does not depend on the location of the word
		\item If we consider the sentence as a sequence in time, they are time-invariant
		MLE picks up the word that is $\dfrac{n_{w,w'}}{n_{w,o}}$ where $n{w,w'}$ is the number of times the words $w_1,w'$ occur together and $n_{w,o}$ is the number of times the word $w$ appears in the bigram sequence with any other word
		\item The number of parameters to be estimated  $=\left|V\right|\times (\left|V\right| + 1)$
	\end{itemize}
\end{frame}


%\begin{frame}{n-Gram Language Model}
%Estimation of the probability of a word given the history os sequence of words. Consider the following sentence
%\vspace{0.3cm}
%My wallet is like an onion, opening it makes me cry
%\vspace{0.3cm}
%P(cry|My wallet is like an onion, opening it makes me)
%\linebreak
%Unigram - P(cry) at the end of the sentence
%\linebreak
%Bigram - P(cry|me)
%\linebreak
%Trigram - P(cry|makes me)
%\linebreak
%The Markov assumption: Future word depends on the recent history
%\end{frame}
%
%\begin{frame}{Probabilistic Language Model}
%	\begin{itemize}
%		\item The probability of the next word is controlled by the context window of words
%		\item Not possible to remember the entire history to predict the next word
%	\end{itemize}
%
%Hence we may use the \textbf{\textit{Markov}} assumption to compute the likelihood of the next word in a sentence:
%	\begin{align}
%		P(w_1,w_2,w_3,...,w_n) &= \displaystyle{\prod_{i=1}^{n} P(w_i|w_1,w_2,w_3,...,w_{i-1})}\\
%			&\approx \displaystyle{\prod_{i=1}^{n} P(w_i|w_{i-3},w_{i-2},w_{i-1})}
%	\end{align}
%
%
%\end{frame}
\subsection{Bigram Language Model - Example}
\begin{frame}{COVID19 Corpus}
The bigram probabilities are estimated using a COVID19 \href{https://drive.google.com/file/d/1PVA5IR4bAn7RulAeuSgxRBNMtmRGXaUm/view?usp=share_link}{corpus}\footnote{https://drive.google.com/file/d/1PVA5IR4bAn7RulAeuSgxRBNMtmRGXaUm/view?usp=share\_link}. The corpus was created by reading the abstracts of all the papers found in the corpus.
\small
\begin{align}
\text{Total number of abstracts} &= 224672\nonumber\\
\text{The  vocabulary count }  |V| &= 249737 \nonumber
\end{align}

\end{frame}

\begin{frame}{Bigram Language Model - Example}
	\small

	\begin{minipage}{0.32\linewidth}
		\begin{tabular}{|lc|}
			\hline
			Context - \textit{\textbf{$<s>$}}&\\
			\hline
			$p( the | <s> )$ & 0.13 \\
			$p( in | <s> )$ & 0.05 \\
			$p( we | <s> )$ & 0.05 \\
			$p( this | <s> )$ & 0.03 \\
			$p( a | <s> )$ & 0.02 \\
			$p( however | <s> )$ & 0.02 \\
			$p( these | <s> )$ & 0.02 \\
			$p( to | <s> )$ & 0.01 \\
			$p( our | <s> )$ & 0.01 \\
			$p( it | <s> )$ & 0.01 \\
			\hline
		\end{tabular}
	\end{minipage}
	\begin{minipage}{0.32\linewidth}
		\begin{tabular}{|lc|}
			\hline
			Context - \textit{\textbf{we}}&\\
			\hline
			$p( have | we )$ & 0.04 \\
			$p( also | we )$ & 0.04 \\
			$p( found | we )$ & 0.04 \\
			$p( show | we )$ & 0.03 \\
			$p( present | we )$ & 0.03 \\
			$p( propose | we )$ & 0.03 \\
			$p( report | we )$ & 0.03 \\
			$p( describe | we )$ & 0.02 \\
			$p( used | we )$ & 0.02 \\
			$p( identified | we )$ & 0.02 \\
			\hline
		\end{tabular}
	\end{minipage}
	\begin{minipage}{0.32\linewidth}
		\begin{tabular}{|lc|}
			\hline
			Context - \textit{\textbf{report}}&\\
			\hline
			$p( the | report )$ & 0.18 \\
			$p( a | report )$ & 0.1 \\
			$p( of | report )$ & 0.07 \\
			$p( on | report )$ & 0.06 \\
			$p( that | report )$ & 0.06 \\
			$p( we | report )$ & 0.05 \\
			$p( here | report )$ & 0.02 \\
			$p( . | report )$ & 0.02 \\
			$p( describes | report )$ & 0.02 \\
			$p( is | report )$ & 0.02 \\
			\hline
		\end{tabular}
	\end{minipage}

	The bigram probabilities are computed as follows:
	\begin{align}
		P(w_n|w_{n-1}) &= \dfrac{Count(w_{n-1}w_n)}{Counr(w_{n-1})}\\
		P_{L}(w_n|w_{n-1}) &= \dfrac{Count(w_{n-1}w_n) + 1}{Count(w_{n-1})+|V|}
	\end{align}
where $P_L$ is the Laplacian smoothing and $|V|$ is the size of the vocabulary.
	%\end{table}
\end{frame}

%\begin{frame}[shrink=5]{Probabilistic Language Model - Example}
%
%	\begin{align}
%		\noindent &P(Peter|\left<S\right>)*)P(Piper|Peter)*P(picked|Piper)*P(\left<E\right>|picked) \, is\\
%		&P\left(\frac {Peter \, \left<S\right>}{\left<S\right>}\right)
%		\times
%		P\left(\frac{Piper\, Peter}{Peter}\right)
%		\times
%		P\left( \frac{picked\, Piper}{Piper}\right)
%		\times
%		P\left(\frac{\left<E\right>\, picked}{picked}\right) = 0.25*1*0.75*0.33\\
%		&P(Peter|\left<S\right>)*P(Piper|Peter)*P(Piper|picked)*P(peppers|picked)*\\&P(peppers|\left<E\right>) \, is\\
%		&\frac{Peter \, \left<S\right>}{\left<S\right>}*\frac{Piper\,Peter}{Peter}*\frac{picked\,Piper}{Piper}*\frac{peppers\,picked}{picked}*\frac{\left<E\right> \,peppers}{peppers} = ?\\
%		&\text{Will trigrams provide better prediction or generate semi-realistic sentences?}\\
%		&P(Piper|\left<S\right>\,Peter )*P(picked|Peter \,Piper)*P(Peppers|Piper \,picked)*\\
%		&P(\left<E\right>|picked \,peppers) =
%		\frac{\left<S\right>\,Peter\,Piper}{\left<S\right>\,Peter}*\frac{Peter \,Piper\,picked}{Peter\,Piper}*\\
%		&\frac{Piper\,picked\,peppers }{Piper\, picked}*\frac{picked\,peppers\,\left<E\right>}{picked\,peppers}
%	\end{align}
%
%
%\end{frame}

\begin{frame}{Building a Bigram model - Code}

\lstinputlisting{../../Code/Python/BigramLM_part1.py}
\end{frame}

\begin{frame}{Building a Bigram model - Code}

\lstinputlisting{../../Code/Python/BigramLM_part2.py}
\end{frame}

\begin{frame}{Model Parameters - Bigram example}\begin{figure}
	\centering
	\includegraphics[width=1\linewidth]{../Images/bigram_model_parameters}
	\label{fig:modelparameters}
\end{figure}

\end{frame}

\begin{frame}{Bigram model - Next word prediction}
\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{../Images/bigram_fig1.pdf}
	\label{fig:bigrammodelgraphforhowphysics}
\end{figure}

\end{frame}




\subsection{Perplexity}
\begin{frame}{Perplexity}
Perplexity is a measurement of how well a probability model predicts a sample.
Perplexity is defined as
\begin{align}
\text{For bigram model, } PP(W_N) &=  \sqrt[N]{{\prod_{i=1}^{N}}\frac{1}{P(w_i|w_{i-1})}}\\
\text{For trigram model } PP(W_N) &=  \sqrt[N]{{\prod_{i=1}^{N}}\frac{1}{P(w_i|w_{i-1}w_{i-2})}}
\end{align}
A good model gives maximum probability to a sentence or  minimum perplexity to a sentence. It is normalized by the number of words in the sentence $N$.
\end{frame}

\begin{frame}{Unknown Words}
\begin{itemize}
	\item In a closed vocabulary language model, there is no unknown words or \textbf{\textit{out of vocabulary words (OOV)}}
	\item In an open vocabulary system, you will find new words that are not present in the trained model
	\item Pick words below certain frequency and replace them as OOV.
	\item Treat every OOV as a regular word
	\item During testing, the new words would be treated as OOV and the corresponding frequency will be used for computation
	\item This eliminates zero probability for sentences containing OOV

\end{itemize}

\end{frame}
%\subsection{Curse of dimensions}
%\begin{frame}{Curse of Dimensionality}
%\begin{itemize}
%	\item A fundamental problem that makes  language modeling and other learning problems difficult is the curse of dimensionality
%	\item It is particularly obvious in the case when one wants to model the joint distribution between many discrete random variable
%	\item If one wants to estimate the joint probability distribution of $10$ words in a language with a million words as vocabulary, then we need to estimate $10000000^{10}-1 = 10^{60}-1$ free parameters
%\end{itemize}
%
%\end{frame}

%\begin{frame}{Exercise}
%
%\begin{itemize}
%	\item Extend the program\footnote{\url{https://github.com/Ramaseshanr/anlp/blob/master/BigramLM.ipynb}}\footnote{\url{https://github.com/Ramaseshanr/anlp/blob/master/TrigramLM.ipynb}} that predicts the word into another program that computes the probability and perplexity of a test sentence
%	\begin{enumerate}
%		\item Build the language model using corpus
%		\begin{itemize}
%			\item Big corpus $\rightarrow$ Long time to build a model
%			\item Development $\rightarrow$ Choose a smaller corpus
%			\item Testing/Production $\rightarrow$ use a bigger corpus to build your model
%		\end{itemize}
%		\item Check the model parameters using the debugger
%		\item Once satisfied with the learned model, test your sentences using the model
%	\end{enumerate}
%	\item \textbf{Input}:
%	\begin{enumerate}
%		\item A sentence consisting of words in the corpus that you have used for creating the language model
%		\item A sentence with one more words that are OOV
%	\end{enumerate}
%	\item \textbf{Output}:Probability and perplexity of the input sentence
%	\item Try this exercise for trigram and 4-gram language models
%\end{itemize}
%
%
%\end{frame}